# elastic

Рассмотрим, как строка **"Врач УЗИ"** будет индексироваться с использованием различных токенизаторов в Elasticsearch. Каждый токенизатор разделяет текст по-разному, что влияет на то, как будет происходить индексация и как будут выглядеть токены в индексе. Давайте рассмотрим несколько популярных вариантов токенизаторов:

### 1. **`standard` (Стандартный токенизатор)**

**Как работает**:
- Стандартный токенизатор разделяет строку на токены, используя пробелы, знаки препинания и другие разделители как границы.
- В данном случае, строка **"Врач УЗИ"** будет разделена на два токена: **"Врач"** и **"УЗИ"**.

**Пример индексации**:
- Токены: **["Врач", "УЗИ"]**
- Индексирование: 
  - "Врач УЗИ" будет сохранено как два отдельных токена: `"Врач"` и `"УЗИ"`.
  - Пробел будет использоваться как разделитель для токенов.

**Использование**:
- Этот токенизатор подходит для обычных текстов, где важно учитывать разделение слов.
- Запрос **"врач узи"** будет точно совпадать с индексом.

### 2. **`whitespace` (Токенизатор по пробелам)**

**Как работает**:
- Токенизатор `whitespace` делит строку только по пробелам. Он не разделяет по знакам препинания или другим символам.
- В данном случае строка **"Врач УЗИ"** будет разделена на два токена: **"Врач"** и **"УЗИ"**.

**Пример индексации**:
- Токены: **["Врач", "УЗИ"]**
- Индексирование: "Врач УЗИ" разделится на два токена **"Врач"** и **"УЗИ"**, и пробел будет использоваться как разделитель.

**Использование**:
- Подходит для строк, где важны только пробелы для разделения слов.

### 3. **`edge_ngram` (Токенизатор с префиксами)**

**Как работает**:
- Токенизатор **`edge_ngram`** создает токены из начальных частей слова, начиная с определенного количества символов.
- Для строки **"Врач УЗИ"** токенизатор сгенерирует токены, представляющие префиксы этих слов.

**Пример индексации**:
- Если вы установите **`min_gram`** на 2 и **`max_gram`** на 5, то из строки **"Врач УЗИ"** будут созданы токены, как например:
  - "Вр", "Вра", "Врач"
  - "УЗ", "УЗИ"

- Токены: **["Вр", "Вра", "Врач", "УЗ", "УЗИ"]**
- Индексирование: Пробелы сохраняются, но при разбиении на префиксы пробел будет восприниматься как разделитель, и слова будут разбиты на более мелкие части.

**Использование**:
- Этот токенизатор полезен для автодополнения, когда необходимо искать по части слова.
- Запрос "врач узи" может совпасть с любым из токенов: "Вр", "Вра", "Врач", "УЗ", "УЗИ".

### 4. **`ngram` (Токенизатор с n-граммами)**

**Как работает**:
- Токенизатор **`ngram`** генерирует все возможные n-граммы (последовательности из нескольких символов) из исходного текста.
- Для строки **"Врач УЗИ"** будет создано несколько токенов, которые содержат части этих слов.

**Пример индексации**:
- Если вы установите **`min_gram`** на 2 и **`max_gram`** на 5, то из строки **"Врач УЗИ"** будут созданы следующие токены:
  - "Вр", "Вра", "Врач", "У", "УЗ", "УЗИ", "ЗИ"
  
- Токены: **["Вр", "Вра", "Врач", "У", "УЗ", "УЗИ", "ЗИ"]**
- Индексирование: Все возможные последовательности символов длиной от 2 до 5 будут сохранены, включая части "Врач" и "УЗИ".

**Использование**:
- Этот токенизатор полезен для поиска по части слова или при реализации автодополнения.
- Запрос "врач узи" может совпасть с любым из токенов: "Вр", "Вра", "Врач", "У", "УЗ", "УЗИ", "ЗИ".

### 5. **`keyword` (Токенизатор ключевых слов)**

**Как работает**:
- Токенизатор **`keyword`** не выполняет никакой токенизации, а рассматривает весь ввод как одно целое слово.
- Для строки **"Врач УЗИ"** будет создан один токен: **"Врач УЗИ"**.

**Пример индексации**:
- Токены: **["Врач УЗИ"]**
- Индексирование: Вся строка "Врач УЗИ" будет проиндексирована как один токен без разделения на слова.

**Использование**:
- Этот токенизатор полезен, если нужно хранить и искать точные фразы, не разделяя их на составные части.
- Запрос **"врач узи"** будет искать точное совпадение с фразой **"Врач УЗИ"**.

---

### Резюме

Вот как будет индексироваться строка **"Врач УЗИ"** с различными токенизаторами:

| Токенизатор   | Индексация (Токены)              | Примечания |
|---------------|----------------------------------|------------|
| **`standard`** | ["Врач", "УЗИ"]                  | Стандартный токенизатор делит по пробелам и знакам препинания. |
| **`whitespace`** | ["Врач", "УЗИ"]                  | Разделяет только по пробелам. |
| **`edge_ngram`** | ["Вр", "Вра", "Врач", "УЗ", "УЗИ"] | Разделяет на префиксы слов, полезен для автодополнения. |
| **`ngram`**     | ["Вр", "Вра", "Врач", "У", "УЗ", "УЗИ", "ЗИ"] | Создает n-граммы для поиска по частям слов. |
| **`keyword`**   | ["Врач УЗИ"]                     | Хранит строку как одно целое слово, без разделения. |

### Какой токенизатор выбрать?

- Для **обычного текста** (например, поисковых запросов) используйте **`standard`** или **`whitespace`** токенизатор.
- Для **поиска с префиксами** или **автодополнения** используйте **`edge_ngram`**.
- Для **поиска по частям слов** или если нужно искать последовательности символов, используйте **`ngram`**.
- Для **точных фраз** используйте **`keyword`**, чтобы хранить и искать без изменений.

Таким образом, выбор токенизатора зависит от специфики вашего поиска (например, автодополнение, точные фразы или поиск по частям слов).
