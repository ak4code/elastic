Таблица сравнения работы разных токенизаторов с текстом "Стоматолог-хирург Ёванов Ёван - оглы <3%>"


---

Подробности

1. standard токенизатор

Разделяет строку на токены по пробелам, дефисам, пунктуации.

Очищает текст от спецсимволов (<, %, >).

Подходит для поиска по словам, но не для текстов с дефисами, спецсимволами.

Запросы:

"Стоматолог-хирург" — не найдет, так как строка разбита на два слова.

"хирург" — найдет.




---

2. whitespace токенизатор

Разделяет строку только по пробелам, сохраняя дефисы, спецсимволы.

Подходит для поиска по фразам, содержащим дефисы и символы.

Запросы:

"Стоматолог-хирург" — найдет, так как сохраняет токен как есть.

"-" — найдет, так как дефис становится отдельным токеном.




---

3. char_group токенизатор

Настраивается для работы с конкретными символами.

Например, если удалять спецсимволы <, %>, результат похож на standard.

Запросы:

"Стоматолог-хирург" — не найдет, так как дефисы удалены.

"-" — не найдет.




---

4. ngram токенизатор (min_gram=1, max_gram=5)

Создает последовательности длиной от 1 до 5 символов.

Индексирует каждую часть текста, включая буквы, цифры, спецсимволы.

Подходит для автодополнения и поиска по частям текста.

Запросы:

"Стоматолог-хирург" — найдет по совпадению с n-граммами.

"-" — найдет, так как дефис включен как отдельная n-грамма.

"<3%>" — найдет, так как символы индексируются.




---

5. edge_ngram токенизатор (min_gram=1, max_gram=5)

Похож на ngram, но создает токены только от начала каждого слова.

Подходит для автодополнения.

Запросы:

"Стоматолог-хирург" — найдет по началу слов.

"-" — найдет, так как дефис включен как отдельный токен.

"<3%>" — найдет.




---

6. keyword токенизатор

Сохраняет строку без изменений как единый токен.

Подходит для точного совпадения строки.

Запросы:

"Стоматолог-хирург" — не найдет, если запрос не совпадает точно.

"-" — не найдет.




---

Рекомендации для проекта

1. Для гибкости: Используйте ngram или edge_ngram с min_gram=1, чтобы находить частичные совпадения по дефисам, спецсимволам и отдельным частям слов.


2. Для точного поиска: Если нужны фразы с дефисами, пробелами и спецсимволами, настройте whitespace токенизатор.


3. Пример настроек индекса:



{
  "settings": {
    "analysis": {
      "tokenizer": {
        "custom_ngram": {
          "type": "ngram",
          "min_gram": 1,
          "max_gram": 5,
          "token_chars": ["letter", "digit", "punctuation", "symbol"]
        }
      },
      "analyzer": {
        "custom_analyzer": {
          "type": "custom",
          "tokenizer": "custom_ngram",
          "filter": ["lowercase", "russian_morphology"]
        }
      }
    }
  }
}

4. Тест анализатора:



POST /_analyze
{
  "analyzer": "custom_analyzer",
  "text": "Стоматолог-хирург Ёванов Ёван - оглы <3%>"
}

Эта настройка обеспечит поиск как по целым словам, так и по их частям, включая символы и дефисы.

