Подробное описание работы каждого токенайзера с текстом "Стоматолог-хирург Ёванов Ёван - оглы <3%>"


---

1. standard токенизатор

Описание:
standard токенизатор — это стандартный токенайзер ElasticSearch. Он разделяет текст по пробелам, дефисам, пунктуации и спецсимволам. После разбиения удаляет все небуквенные и нецифровые символы.

Токены:

["Стоматолог", "хирург", "Ёванов", "Ёван", "оглы", "3"]

Особенности:

Дефисы и спецсимволы удаляются.

Подходит для общего текстового поиска, но плохо обрабатывает сложные конструкции с дефисами и символами.


Пример работы с запросами:

Запрос "Стоматолог-хирург" → Не найдет (разделяет на два токена).

Запрос "хирург" → Найдет.

Запрос "-" → Не найдет.

Запрос "<3%>" → Не найдет.




---

2. whitespace токенизатор

Описание:
whitespace токенизатор разбивает текст только по пробелам, оставляя дефисы, символы и спецсимволы.

Токены:

["Стоматолог-хирург", "Ёванов", "Ёван", "-", "оглы", "<3%>"]

Особенности:

Сохраняет дефисы, спецсимволы и их комбинации.

Хорошо подходит для поиска по фразам, включающим символы и дефисы.


Пример работы с запросами:

Запрос "Стоматолог-хирург" → Найдет.

Запрос "хирург" → Не найдет (части слов не индексируются).

Запрос "-" → Найдет.

Запрос "<3%>" → Найдет.




---

3. char_group токенизатор

Описание:
char_group токенизатор позволяет точно задавать правила, по каким символам разбивать текст, и какие символы оставлять. Например, можно настроить токенайзер так, чтобы сохранять дефисы и спецсимволы.

Настройка (пример):

{
  "type": "char_group",
  "tokenize_on_chars": [
    "whitespace", "-", "<", ">", "%", "letter", "digit"
  ]
}

Токены (с приведенной настройкой):

["Стоматолог", "хирург", "Ёванов", "Ёван", "оглы", "3"]

Особенности:

Разделяет строку по пробелам и указанным символам (<, %, и др.).

Удаляет ненужные символы, сохраняя только заданные.


Пример работы с запросами:

Запрос "Стоматолог-хирург" → Не найдет (дефис удален).

Запрос "хирург" → Найдет.

Запрос "-" → Найдет (если указано в настройке).

Запрос "<3%>" → Найдет (если включены символы <, %).




---

4. ngram токенизатор

Описание:
ngram токенизатор разбивает текст на последовательности символов заданной длины. Например, с min_gram=1 и max_gram=5 он создаст токены длиной от 1 до 5 символов.

Настройка:

{
  "type": "ngram",
  "min_gram": 1,
  "max_gram": 5,
  "token_chars": ["letter", "digit", "punctuation", "symbol"]
}

Токены (частично):

["С", "Ст", "Стом", "Стомат", "о", "ом", "ома", ..., "<", "<3", "<3%"]

Особенности:

Создает n-граммы из каждого слова или символа.

Подходит для автодополнения и поиска по частям текста.

Сохраняет спецсимволы и дефисы, если включены в token_chars.


Пример работы с запросами:

Запрос "Стоматолог-хирург" → Найдет (по частям строки).

Запрос "хирург" → Найдет.

Запрос "-" → Найдет.

Запрос "<3%>" → Найдет.




---

5. edge_ngram токенизатор

Описание:
edge_ngram токенизатор похож на ngram, но создает токены только от начала каждого слова.

Настройка:

{
  "type": "edge_ngram",
  "min_gram": 1,
  "max_gram": 5,
  "token_chars": ["letter", "digit", "punctuation", "symbol"]
}

Токены (частично):

["С", "Ст", "Стом", "Стомат", "х", "хи", "хир", "хиру", ..., "<", "<3", "<3%"]

Особенности:

Создает токены только от начала каждого слова.

Подходит для автодополнения по первым буквам.

Сохраняет дефисы и символы, если включены в token_chars.


Пример работы с запросами:

Запрос "Стоматолог-хирург" → Найдет (по началу строки).

Запрос "хирург" → Найдет.

Запрос "-" → Найдет.

Запрос "<3%>" → Найдет.




---

6. keyword токенизатор

Описание:
keyword токенизатор сохраняет всю строку как единый токен. Подходит для точного поиска.

Токен:

"Стоматолог-хирург Ёванов Ёван - оглы <3%>"

Особенности:

Полностью сохраняет строку.

Используется для точного поиска совпадений.


Пример работы с запросами:

Запрос "Стоматолог-хирург" → Не найдет (строка не совпадает).

Запрос "хирург" → Не найдет.

Запрос "-" → Не найдет.

Запрос "<3%>" → Не найдет.

